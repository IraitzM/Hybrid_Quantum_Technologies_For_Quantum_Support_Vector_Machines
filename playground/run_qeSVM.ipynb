{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Support Vector Machines on the D-Wave Quantum Annealer\n",
    "#### Created by Gabriele Cavallaro (g.cavallaro@fz-juelich.de)\n",
    "#### Modified for the paper __Hybrid Quantum Technologies For Quantum Support Vector Machines__ by Filippo Orazi (filippo.orazi2@unibo.it)\n",
    "\n",
    "### Change the name of the dataset to reproduce the experiments, \n",
    "You should create the folder tree as below \n",
    "\n",
    "current_folder\n",
    "- input_datasets\n",
    "  - calibration\n",
    "    - dataset_name1\n",
    "    - dataset_name2\n",
    "  - train\n",
    "    - dataset_name1\n",
    "    - dataset_name2\n",
    "- outputs\n",
    "  - calibration\n",
    "    - dataset_name1\n",
    "    - dataset_name2\n",
    "  - train\n",
    "    - dataset_name1\n",
    "    - dataset_name2\n",
    "  - test\n",
    "    - dataset_name1\n",
    "    - dataset_name2\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0. Setting Up the Access to the D-Wave 2000Q quantum computer\n",
    "\n",
    "- Make a free account to run on the D-Wave through [Leap](https://www.dwavesys.com/take-leap)\n",
    "\n",
    "- Install Ocean Software with [pip install dwave-ocean-sdk](https://docs.ocean.dwavesys.com/en/latest/overview/install.html)\n",
    "\n",
    "- Configuring the D-Wave System as a Solver with [dwave config create](https://docs.ocean.dwavesys.com/en/latest/overview/dwavesys.html#dwavesys)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Data Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.1 Load of the Python Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import * # It contains functions for threat the data (I/O, encoding/decoding) and metrics for evaluations \n",
    "from quantum_SVM import *\n",
    "from sklearn import preprocessing\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.model_selection import train_test_split\n",
    "from quantum_SVM import *\n",
    "import numpy as np\n",
    "from utils import *\n",
    "from sklearn.model_selection import KFold\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2 Select the Dataset\n",
    "\n",
    "##### In this notebook we consider the datasets of [HyperLabelMe](http://hyperlabelme.uv.es/index.html) (i.e., a benchmark system for remote sensing image classification). \n",
    "\n",
    "- It contains 43 image datasets, both multi- and hyperspectral\n",
    "- For each one, training pairs (spectra and their labels) and test spectra are provided\n",
    "- The test labels are not given. The predicted labels needs to be uploaded in HyperLabelMe which will return the accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data\n",
    "# in this instance gamma represents the number of features in the dataset\n",
    "gamma=2\n",
    "\n",
    "id_dataset='MNIST56'\n",
    "[X_train,X_test, Y_train, Y_test ]=load(id_dataset, gamma)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.5 Quantum SVM: Calibration Phase\n",
    "\n",
    "The SVM on the QA depends on four hyperparameters:\n",
    "the encoding base $B$, the number $K$ of qubits per coefficient $\\alpha_{n}$, the multiplier $\\xi$, and the kernel parameter $\\gamma$. The parameter $n_{cpl}$ varies for each run  and is not a parameter of the SVM itself. \n",
    "\n",
    "The hyperparameters are selected through a 4-fold cross-validation. Each training set includes only 120 samples (i.e., choice due to the limitations of the QA). The validation includes the remaining samples that are used for the evaluation of the performance.  \n",
    "\n",
    "For each dataset, the values are calibrated by evaluating the SVM for $B \\in \\{2, 10\\}$, $K\\in \\{2, 3\\}$, and $\\xi \\in \\{1,2\\}$  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 10-fold Monte Carlo (or split-and-shuffle) cross-validation\n",
    "fold=4\n",
    "\n",
    "for i in range(0,fold):\n",
    "    X_train_cal, X_val_cal, Y_train_cal, Y_val_cal = train_test_split(X_train,Y_train, test_size=0.6, random_state=i)\n",
    "    \n",
    "    # Pre-processing \n",
    "    # X_train_cal = preprocessing.scale(X_train_cal)\n",
    "    # X_val_cal = preprocessing.scale(X_val_cal)\n",
    "    a = list(map(list, zip(*X_train_cal)))\n",
    "    \n",
    "\n",
    "   # Write the data\n",
    "    write_samples(X_train_cal, Y_train_cal,'input_datasets/calibration/'+id_dataset+'/'+id_dataset+'calibtrain'+str(i))\n",
    "    write_samples(X_val_cal, Y_val_cal,'input_datasets/calibration/'+id_dataset+'/'+id_dataset+'calibval'+str(i))\n",
    "    \n",
    "print('Each training set includes '+str(X_train_cal.shape[0])+ ' samples')\n",
    "print('Each validation set includes '+str(X_val_cal.shape[0])+ ' samples')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Hyperparameters \n",
    "K=[2,3]\n",
    "B=[2, 10]\n",
    "xi=[1,2]\n",
    "n_experiments=len(B)*len(K)*len(xi)\n",
    "\n",
    "\n",
    "hyperparameters=np.zeros([n_experiments,4], dtype=float)\n",
    "\n",
    "path_data_key='input_datasets/calibration/'+id_dataset+'/'\n",
    "data_key = id_dataset+'calibtrain'\n",
    "path_out='outputs/calibration/'+id_dataset+'/'\n",
    "\n",
    "trainacc=np.zeros([fold], dtype=float)\n",
    "trainauroc=np.zeros([fold], dtype=float)\n",
    "trainauprc=np.zeros([fold], dtype=float)\n",
    "    \n",
    "testacc=np.zeros([fold], dtype=float)\n",
    "testauroc=np.zeros([fold], dtype=float)\n",
    "testauprc=np.zeros([fold], dtype=float)\n",
    "\n",
    "trainacc_all=np.zeros([n_experiments], dtype=float)\n",
    "trainauroc_all=np.zeros([n_experiments], dtype=float)\n",
    "trainauprc_all=np.zeros([n_experiments], dtype=float)\n",
    "    \n",
    "testacc_all=np.zeros([n_experiments], dtype=float)\n",
    "testauroc_all=np.zeros([n_experiments], dtype=float)\n",
    "testauprc_all=np.zeros([n_experiments], dtype=float)\n",
    "\n",
    "\n",
    "f = open(path_out+'calibration_results.txt',\"w\") \n",
    "f.write(\"B\\t K\\t xi\\t   gamma\\t trainacc\\t trainauroc\\t trainauprc\\t testacc\\t testauroc\\t testauprc\\n\") \n",
    "  \n",
    "count=0 \n",
    "for x in range(0,len(B)):\n",
    "    for y in range(0,len(K)):\n",
    "        for z in range(0,len(xi)):\n",
    "            for i in range(0,1): # here the author loop over gamma that is fixed in our implementation\n",
    "                for j in range(0,fold):\n",
    "                    path=gen_svm_qubos(B[x],K[y],xi[z],gamma,path_data_key,data_key+str(j),path_out)\n",
    "                    pathsub=dwave_run(path_data_key,path)\n",
    "                    [trainacc[j],trainauroc[j],trainauprc[j],testacc[j],testauroc[j],testauprc[j]]=eval_run_rocpr_curves(path_data_key,pathsub,'noplotsave')\n",
    "                    \n",
    "                hyperparameters[count,0]=B[x]\n",
    "                hyperparameters[count,1]=K[y]\n",
    "                hyperparameters[count,2]=xi[z]\n",
    "                hyperparameters[count,3]=gamma\n",
    "            \n",
    "                trainacc_all[count]=np.average(trainacc)\n",
    "                trainauroc_all[count]=np.average(trainauroc)\n",
    "                trainauprc_all[count]=np.average(trainauprc)\n",
    "    \n",
    "                testacc_all[count]=np.average(testacc)\n",
    "                testauroc_all[count]=np.average(testauroc)\n",
    "                testauprc_all[count]=np.average(testauprc)\n",
    "                \n",
    "                np.save(path_out+'hyperparameters', hyperparameters)\n",
    "                np.save(path_out+'trainacc_all', trainacc_all)\n",
    "                np.save(path_out+'trainauroc_all', trainauroc_all)\n",
    "                np.save(path_out+'trainauprc_all', trainauprc_all)\n",
    "                np.save(path_out+'testacc_all', testacc_all)\n",
    "                np.save(path_out+'testauroc_all', testauroc_all)\n",
    "                np.save(path_out+'testauprc_all', testauprc_all)\n",
    "                \n",
    "                f.write(f'{B[x]}\\t {K[y]}\\t {xi[z]}\\t {gamma:8.3f}\\t {np.average(trainacc):8.4f}\\t {np.average(trainauroc):8.4f}\\t {np.average(trainauprc):8.4f}\\t {np.average(testacc):8.4f}\\t {np.average(testauroc):8.4f}\\t {np.average(testauprc):8.4f}')\n",
    "                f.write(\"\\n\") \n",
    "                count=count+1\n",
    "                \n",
    "f.close() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.6 Quantum SVM: Training Phase\n",
    "\n",
    "To overcome the problem of the limited connectivity of the hardware the whole training set is split into small disjoint subsets $D^{(train,l)}$ of $~50$ samples, with $l=0,...,int(N/50)$. \n",
    "The strategy is to build an ensemble of quantum weak SVMs (qeSVMs) where each classifier is trained on $D^{(train,l)}$. \n",
    "This is achieved in two steps. First, for each subset $D^{(train,l)}$ the twenty best solutions from the annealer (i.e., qSVM$(B, K, \\xi , \\gamma )\\#i$ for $i =0, ... ,19$) are combined by averaging over the respective decision functions $f^{l,i}(\\mathbf{x})$ (see Eq. (3)). \n",
    "\n",
    "Since the decision function is linear in the coefficients\n",
    "and the bias $b^{(l,i)}$ is computed from $\\alpha_{n}^{(l,i)}$ via Eq. (4), this procedure effectively results in one classifier with an effective set of coefficients \n",
    "$\\alpha_{n}^{(l)}=\\sum_{i} \\alpha_{n}^{(l, i)} / 20$ and bias \n",
    "$b^{l}=\\sum_{i} b^{(l, i)} / 20$.\n",
    "Second, an average is made over the $int(N/50)$ subsets. \n",
    "\n",
    "Note, however, that the data points \n",
    "$\\left(\\mathbf{x}_{n}^{(l)}, y_{n}^{(l)}\\right) \\in D^{(\\text {train }, l)}$ are now different for each $l$. The full decision function is\n",
    "\n",
    "\\begin{equation}\n",
    "F(\\mathbf{x})=\\frac{1}{L} \\sum_{n l} \\alpha_{n}^{(l)} y_{n}^{(l)} k\\left(\\mathbf{x}_{n}^{(l)}, \\mathbf{x}\\right)+b,\n",
    "\\end{equation}\n",
    "\n",
    "where $b=\\sum_{l} b^{(l)} / L$. As before, the decision for the class label of a point $\\mathbf{x}$ is obtained through $\\widetilde{t}=\\operatorname{sign}(F(\\mathbf{x}))$. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Training parameters\n",
    "experiments=1\n",
    "slice=50 # Number of samples to use for the training\n",
    "fold=int(len(X_train)/slice)\n",
    "\n",
    "# Write the data\n",
    "\n",
    " \n",
    "for i in range(0,experiments):    \n",
    "    cv = KFold(n_splits=fold, random_state=i, shuffle=True)\n",
    "    count=0\n",
    "    for test_index, train_index in cv.split(X_train):\n",
    "        #print(\"Train Index: \", len(train_index), \"\\n\")\n",
    "        \n",
    "        X_train_slice, y_train_slice = X_train[train_index], Y_train[train_index]\n",
    "        #X_train_slice = preprocessing.scale(X_train_slice)\n",
    "        \n",
    "        X_test_slice, y_test_slice = X_train[test_index], Y_train[test_index]\n",
    "        #X_test_slice = preprocessing.scale(X_test_slice)\n",
    "        \n",
    "        write_samples(X_train_slice, y_train_slice,f'input_datasets/train/'+id_dataset+'/'+id_dataset+'calibtrain'+str(i)+'_'+str(count))\n",
    "        write_samples(X_test_slice, y_test_slice,f'input_datasets/train/'+id_dataset+'/'+id_dataset+'calibval'+str(i)+'_'+str(count))\n",
    "        \n",
    "        count=count+1\n",
    "print(f'the training set has {fold} folds')\n",
    "print(\"Each training set has\", len(train_index), \"samples\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the calibration results\n",
    "path_out='outputs/calibration/'+id_dataset+'/'\n",
    "hyperparameters=np.load(path_out+'hyperparameters.npy')\n",
    "testauprc_all=np.load(path_out+'testauprc_all.npy')\n",
    "\n",
    "# Select the best hyperparameter set for the max value of testauprc\n",
    "idx_max = np.where(testauprc_all == np.amax(testauprc_all))\n",
    "B=int(hyperparameters[int(idx_max[0]),0])\n",
    "K=int(hyperparameters[int(idx_max[0]),1])\n",
    "xi=int(hyperparameters[int(idx_max[0]),2])\n",
    "gamma=hyperparameters[int(idx_max[0]),3]\n",
    "print('The best hyperparameters are:\\n'+'B = '+str(B)+' K = '+str(K)+' xi = '+str(xi)+' gamma = '+str(gamma))\n",
    "\n",
    "path_data_key='input_datasets/train/'+id_dataset+'/'\n",
    "data_key = id_dataset+'calibtrain'\n",
    "path_out='outputs/train/'+id_dataset+'/'\n",
    "\n",
    "trained_SVMs=[]\n",
    "\n",
    "for j in range(0,experiments):\n",
    "    for i in range(0,fold):\n",
    "        path=gen_svm_qubos(B,K,xi,gamma,path_data_key,data_key+str(j)+'_'+str(i),path_out)\n",
    "        trained_SVMs.append(dwave_run(path_data_key,path))\n",
    "        np.save(path_out+'trained_SVMs',trained_SVMs)   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.7 Quantum SVM: Test Phase\n",
    " \n",
    "The performance of the qeSVMs can be evaluated directly on [HyperLabelMe](http://hyperlabelme.uv.es/index.html) by uploading the predictions (i.e., output file of the next cell)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_data_key='input_datasets/train/'+id_dataset+'/'\n",
    "data_key = id_dataset+'calibtrain'\n",
    "path_train_out='outputs/train/'+id_dataset+'/'\n",
    "path_test_out='outputs/test/'+id_dataset+'/'\n",
    "\n",
    "path_files=np.load(path_train_out+'trained_SVMs.npy')\n",
    "\n",
    "experiments=1\n",
    "slices=6\n",
    "scores=[]\n",
    "for j in range(0,experiments):\n",
    "    for i in range(0,slices):\n",
    "        scores.append(predict(path_data_key, [i],X_test))\n",
    " \n",
    "avg_scores=np.zeros((scores[0].shape[0]))\n",
    "Y_predicted=np.zeros((scores[0].shape[0]),int)\n",
    "    \n",
    "for i in range(0,scores[0].shape[0]):\n",
    "    tmp=0\n",
    "    for y in range(0,slices):\n",
    "        tmp=tmp+np.sign(scores[y][i])\n",
    "    avg_scores[i]=tmp   \n",
    "\n",
    "\n",
    "for i in range(0,scores[0].shape[0]):\n",
    "    if(avg_scores[i]<0):\n",
    "        Y_predicted[i]=-1\n",
    "    else:\n",
    "        Y_predicted[i]=1\n",
    "        \n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "sns.heatmap(confusion_matrix(Y_predicted, Y_test)/len(Y_predicted),annot=True)\n",
    "classification_report(Y_predicted, Y_test)\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "5f3102f0dbf9f67f56c5d2acad4fc8c28bb1d89a1ba359b417f7abbe5fc8785d"
  },
  "kernelspec": {
   "display_name": "juwels_ker",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
